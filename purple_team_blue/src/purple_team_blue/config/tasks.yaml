#cti_analysis_task:
#  description: >
#    Read and analyse the threat intel report provided with the RAG tool at your disposal.
#    Extract the two most interesting TTPS (or ATT&CK IDs) for the behaviors described in the threat intel report.
#    Then, learn more about these two TTPS by reading through the relevant pages on the official MITRE website (and if needed, other authoritative websites).
#  expected_output: >
#    A structured Markdown document detailing the TTPs, associated MITRE ATT&CK IDs (e.g., T1059.003), and the reasoning for each mapping.
#  agent: intel_analyst
#
#initial_rule_creation_task:
#  description: >
#    Using the structured TTP analysis report from the Intel Analyst, develop a set of robust detection rules in the Sigma format.
#    To do this, iterate through the list of TTPs one by one. For each behavior, generate a robust Sigma detection rule.
#  expected_output: >
#   A report describing how the search went for each TTP, with clear indication of whether any rule was found, what the rule looked like, and if the rule had
#   to be modified or tuned to detect the behaviors described in the report.
#   A YAML file or a collection of YAML files containing valid Sigma rules:
#   - Each rule should be complete, with fields for title, id, status, description,
#   logsource, detection, condition, and relevant tags including the ATT&CK IDs.
#  agent: detection_engineer
#  context:
#  - cti_analysis_task

  
# Sigma Rule Creation Workflow (VERSION 2)

#analyze_ttp:
#  description: >
#    1. Read and analyse the threat intel report provided with the RAG tool at your disposal.
#    2. Extract the two most interesting TTPS (or ATT&CK IDs) for the behaviors described in the threat intel report.
#    3. Then, learn more about these two TTPS by reading through the relevant pages on the official MITRE website (and if needed, other authoritative websites).
#    4. Deconstruct it into its core components. Identify common processes, command-line arguments,
#       parent processes, registry keys, file paths, and the attacker's primary objective.
#    5. Study the `robust examples` to understand what makes a rule effective (specificity, filters).
#  expected_output: >
#    A structured report detailing the observable artifacts, common tools, behaviors, and key detection
#    strategies associated with the selected TTPs, explicitly referencing lessons learned from the
#    good rule examples.
#  agent: threat_analyst
#
#draft_detection_logic:
#  description: >
#    Using the threat analysis report from the previous step, draft the core technical components of a
#    Sigma rule. When using the Knowledge base tool, the 'query' argument must be a simple, concise string question.
#    - Define the most appropriate `logsource` and `category`.
#    - Create specific, multi-conditional `selection` blocks based on the analysis.
#    - Proactively create `filter` blocks to exclude known-good behavior and potential false positives.
#    - Define the `condition` that combines the selections and filters.
#  expected_output: >
#    A YAML snippet containing only the `logsource` and `detection` (with `selection`, `filter`,
#    and `condition`) sections of the Sigma rule. The logic must be precise and robust.
#  context:
#    - analyze_ttp
#  agent: detection_drafter
#
#finalize_sigma_rule:
#  description: >
#    Take the drafted detection logic and create a complete, production-ready Sigma rule.
#    When using the Knowledge base tool, the 'query' argument must be a simple, concise string question.
#    Your primary responsibilities are:
#    1. Further work on the rule to make it as a specific as possible, to avoid raising large amounts of false positives
#    2. Critically review the logic for other potential weaknesses
#    2. Write all required metadata fields: `title`, `id` (use a new UUID), `status` (set to 'experimental'),
#       `description` (be detailed), `references`, `author` (use 'AI Detection Crew'), `date`, `modified`,
#       `tags` (including the TTP ID), `falsepositives` (be specific!), and `level` (e.g., high, medium).
#    3. Format the final output as a single, complete Sigma rule in YAML format, ready to be deployed.
#  expected_output: >
#    A single, complete, well-formatted, and production-ready Sigma rule in YAML format, with all
#    metadata fields thoroughly completed and the logic vetted for quality.
#  context:
#    - draft_detection_logic
#  agent: rule_finalizer


# VERSION 3

analyze_threat_requirements:
  description: >
    Read and analyse the threat intel report provided with the RAG tool at your disposal.
    2. Extract the two most significant TTPS (or ATT&CK IDs) for the behaviors described in the threat intel report.
    3. Then, learn more about these two TTPS by reading through the relevant pages on the official MITRE website (and if needed, other authoritative websites).
    4. Deconstruct it into its core components. Identify common processes, command-line arguments,
      parent processes, registry keys, file paths, and the attacker's primary objective.
    This includes:
    
    1. Threat Classification:
       - Identify ONLY ONE of the described threat types (malware, technique, behavior)
       - Map it to one or more relevant MITRE ATT&CK IDs
       - Determine threat severity level
    
    2. Data Source Analysis:
       - Identify optimal log sources for detection
       - Determine required log fields and event types
       - Consider multiple detection approaches if applicable
    
    3. Detection Strategy:
       - Define detection approach (signature-based, behavioral, statistical)
       - Identify key indicators and artifacts
       - Consider evasion techniques and variants

    **IMPORTANT:** If your tool for finding the current date fails, use today's
     date, September 23, 2025, as a fallback. Do not get stuck. Acknowledge the
     tool failure and proceed with the rest of the task using this fallback date.
    
   # 4. Scope and Context:
   #    - Define detection scope (endpoint, network, cloud)
   #    - Identify environmental considerations
   #    - Note any prerequisites or dependencies
    
   # Input: {threat_description}
   # 
   # Output: A structured threat analysis brief containing all above elements.
  

  expected_output: >
    A comprehensive threat analysis brief in structured format including:
    - Threat classification and MITRE ATT&CK mapping
    - Recommended data sources and log types
    - Detection strategy and key indicators  
    - Scope definition and environmental considerations
    - Specific fields and values to monitor
  agent: threat_analyst_agent
  output_file: "threat_analysis_brief.md"

create_rule_metadata:
  description: >
    Generate comprehensive metadata for the Sigma rule based on the threat analysis brief.
    
    Required metadata elements:
    1. Title: Descriptive, specific title following naming conventions
    2. ID: UUID4 identifier for the rule
    3. Status: Rule maturity level (experimental, test, stable)
    4. Description: Detailed description of what the rule detects
    5. References: Relevant threat intelligence, CVEs, or research links
    6. Author: Rule author information
    7. Date: Creation/modification dates
    8. Modified: Last modification date
    9. Tags: Relevant categorization tags
    10. Level: Severity level (low, medium, high, critical)
    11. MITRE ATT&CK: Technique and tactic mappings
    12. Logsource: Target log source specification (usually it's Windows)
    
    Ensure all metadata follows Sigma rule standards and provides sufficient context 
    for analysts and rule management systems.

  expected_output: >
    Complete Sigma rule metadata section in valid YAML format, including:
    - All required metadata fields properly formatted
    - Accurate MITRE ATT&CK technique mappings
    - Appropriate severity level and tags
    - Proper logsource specification
    - UUID4 identifier and timestamps
  agent: metadata_specialist_agent
  context: 
    - analyze_threat_requirements
  output_file: "metadata.yml"

develop_detection_logic:
  description: >
    Create the complete detection section of the Sigma rule, including selections, 
    filters, and condition logic. This task requires careful coordination of all 
    detection components with special attention to Living Off The Land techniques 
    and legitimate system tool usage patterns.
    
    Components to develop:
    1. Selection Blocks:
       - Define base selections with appropriate field mappings
       - Create variant selections for different attack methods
       - Include LOLBins-specific detection patterns when applicable
       - Ensure proper field naming and value matching
    
    2. Filter Logic:
       - Identify legitimate activities to exclude (especially common admin tasks)
       - Create negative filters to reduce false positives
       - Consider time-based or conditional filtering
       - Account for legitimate LOLBins usage scenarios
    
    3. Condition Statement:
       - Combine selections and filters logically
       - Use appropriate boolean operators (and, or, not)
       - Optimize for performance and accuracy
       - Balance detection coverage with false positive risk
    
    4. Field Specification:
       - List key fields for analyst review
       - Ensure fields align with detection logic
       - Prioritize most relevant investigative fields
       - Include process ancestry and command-line details for LOLBins cases
    
    5. LOLBins Considerations:
       - Identify if the threat involves abuse of legitimate system tools
       - Implement detection patterns that focus on malicious usage indicators
       - Consider parent-child process relationships
       - Analyze command-line arguments and parameters
       - Factor in file locations and network connections
       - Account for normal administrative usage patterns
    
    Technical Requirements:
    - Use proper Sigma syntax and field mappings
    - Consider case sensitivity and wildcards appropriately
    - Optimize for performance while maintaining accuracy
    - Ensure compatibility with common SIEM platforms
    - Implement LOLBins detection best practices when applicable
    
  expected_output: >
    Complete detection section in valid Sigma YAML format containing:
    - Well-structured selection blocks with proper field mappings
    - LOLBins-aware detection patterns when applicable
    - Appropriate filter logic to minimize false positives from legitimate admin activities
    - Logical condition statement combining all elements effectively
    - Relevant fields list for analyst investigation including process details
    - Syntactically correct and performant detection logic
    - Clear documentation of LOLBins techniques addressed (if applicable)
  agent: detection_logic_engineer
  context:
    - analyze_threat_requirements
    - create_rule_metadata

#validate_sigma_rule:
#  description: >
#    Perform comprehensive validation of the complete Sigma rule for syntax correctness, 
#    logical consistency, and adherence to best practices.
#    
#    Validation Areas:
#    1. Syntax Validation:
#       - Check YAML syntax and structure
#       - Validate Sigma-specific syntax elements
#       - Verify field mappings and value formats
#    
#    2. Logical Consistency:
#       - Ensure condition references match selection names
#       - Verify filter logic doesn't contradict selections
#       - Check for logical impossibilities or redundancies
#    
#    3. Best Practices Review:
#       - Assess performance implications
#       - Review field naming conventions
#       - Check for proper use of wildcards and operators
#    
#    4. Completeness Check:
#       - Verify all required sections are present
#       - Ensure metadata completeness and accuracy
#       - Validate proper logsource specification
#    
#    5. Quality Assessment:
#       - Evaluate detection coverage and accuracy
#       - Assess potential for false positives/negatives
#       - Review overall rule effectiveness
#    
#    If issues are identified, provide specific recommendations for improvement and 
#    coordinate with relevant agents for corrections.
#    
#    Input: Complete Sigma rule from all previous tasks
#    
#    Output: Validation report with any issues identified and corrected rule if needed
#  expected_output: >
#    Comprehensive validation report including:
#    - Syntax validation results with any errors identified
#    - Logical consistency assessment
#    - Best practices compliance evaluation
#    - Specific recommendations for improvements
#    - Final validated Sigma rule (corrected if necessary)
#    - Quality score and effectiveness assessment
#  agent: rule_validation_agent
#  context:
#    - analyze_threat_requirements
#    - create_rule_metadata
#    - develop_detection_logic
#  output_file: "draft_sigma_rule.yml"

analyze_false_positives:
  description: >
    Conduct thorough analysis to identify potential false positive scenarios based 
    on real-world Windows system administration and DevOps experience. Leverage 
    deep knowledge of legitimate system operations, scheduled tasks, service accounts, 
    and administrative workflows.
    
    Analysis Areas:
    1. System Administration Activities:
       - Daily/weekly/monthly administrative tasks and their timing
       - Patch management and Windows Update processes
       - Backup and restore operations (including PowerShell-based)
       - Group Policy updates and Active Directory maintenance
       - Certificate management and renewal processes
       - Service account operations and scheduled tasks
    
    2. DevOps and Automation:
       - CI/CD pipeline activities and deployment scripts
       - Infrastructure as Code (PowerShell DSC, Ansible, etc.)
       - Monitoring and alerting scripts
       - Configuration management activities
       - Automated testing and validation scripts
    
    3. Legitimate Tool Usage (LOLBins Context):
       - Normal administrative use of PowerShell, WMI, certutil, etc.
       - Software installation and configuration processes
       - System troubleshooting and diagnostic activities
       - Network connectivity testing and validation
       - File transfer and synchronization operations
    
    4. Environmental and Temporal Factors:
       - Business hours vs. maintenance windows
       - User context (admin accounts vs. service accounts vs. regular users)
       - Domain-joined vs. workgroup systems
       - Server roles and their typical behaviors
       - Geographic and time zone considerations
    
    5. Service Account and System Behaviors:
       - Scheduled task execution patterns
       - Service startup and shutdown sequences
       - System service communications
       - Antivirus and security tool operations
       - Backup agent and monitoring tool activities
    
    6. Real-World Tuning Recommendations:
       - Specific exclusions based on user accounts or groups
       - Time-based filtering for maintenance windows
       - Process ancestry checks to validate legitimate context
       - Command-line argument filtering for known admin tools
       - Network source/destination filtering for internal operations
    
    Provide actionable, specific recommendations that reflect real-world system 
    administration experience and practical knowledge of how Windows environments 
    actually operate in enterprise settings.
    
    Input: Validated Sigma rule and threat analysis brief
    
    Output: Comprehensive false positive analysis with practical tuning recommendations
  expected_output: >
    Detailed false positive analysis document containing:
    - Categorized list of realistic false positive scenarios based on sysadmin experience
    - Specific examples of legitimate Windows operations that might trigger the rule
    - Timing-based considerations (maintenance windows, business hours, patch cycles)
    - Service account and automation-related false positives
    - LOLBins legitimate usage scenarios and how to distinguish them
    - Practical filtering recommendations with specific Sigma syntax examples
    - Environmental tuning guidance for different Windows deployment scenarios
    - Escalation procedures and analyst guidance for investigating triggered alerts
    - Risk assessment for each identified false positive category with mitigation strategies
  agent: false_positives_analyst
  async_execution: false
  context:
    - analyze_threat_requirements
    - develop_detection_logic
  dependencies:
    - develop_detection_logic


generate_atomic_sigma_rules:
  description: >
    Critically analyze all provided threat intelligence and detection logic to create a set of ATOMIC, HIGH-FIDELITY Sigma rules. 
    Your goal is to produce multiple, focused rules, not one large, monolithic rule. 
    Each rule must target a single, specific attacker behavior to ensure it is precise and has a low false-positive rate.

    Core Principles for Rule Creation:
    1. Deconstruct and Isolate Behaviors:
        - Review the threat analysis and drafted logic to identify distinct stages or techniques (TTPs).
        - Examples of distinct behaviors to isolate:
            - Initial Access: Phishing link click leading to a suspicious network connection.
            - Execution: Payload download via a specific LOLBin (e.g., certutil, bitsadmin).
            - Discovery (Local): Enumeration of local users and groups (`net user`, `whoami /groups`).
            - Discovery (Domain): Broad enumeration of domain objects (`dsquery`, `Get-ADUser -Filter *`).
            - Tooling: Execution of a known offensive security tool (`SharpHound`, `Rubeus`).

    2. One Rule Per Behavior:
        - For EACH distinct behavior identified, you must generate a completely separate and self-contained Sigma rule file.
        - DO NOT combine unrelated activities (like a download AND discovery) into a single rule.

    3. Engineer for High Fidelity (Low Noise):
      - Simple & Logical Conditions: The 'condition' field in each rule must be straightforward (e.g., `selection and not filter`). 
        Avoid complex `or` chains that mix different event types.
      - Robust Indicators: Prioritize strong, context-rich indicators over fragile ones. For example, favor specific
        command-line argument patterns over a simple process name that can be easily changed.
      - Targeted Filtering: Each atomic rule must have its own specific filters designed to weed out false positives 
        FOR THAT SPECIFIC BEHAVIOR. Filters for a network rule are different from filters for a process rule.

  expected_output: >
    A final response containing a collection of multiple, separate Sigma rules in YAML format. Each rule must be atomic, targeting one specific behavior, 
    and meticulously engineered with targeted filters to ensure a high signal-to-noise ratio and production-readiness.
  agent: principal_detection_engineer
  async_execution: false
  context:
    - analyze_threat_requirements
    - create_rule_metadata
    - develop_detection_logic
    - analyze_false_positives
  output_file: "final_rules.txt"